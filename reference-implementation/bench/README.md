# Streams Reference Implementation Benchmarker

The purpose of the stuff in this folder is to run a basic pipe-chain benchmark, under a variety of conditions, so that we can compare the results before and after a spec change. Thus, we can make changes to the spec without theorizing about how they impact performance: instead, we just run the tools in this folder and see what the impact was, if any, to get actual data.

## The Benchmark

The benchmark is found in `pipe-chain.js`. It sets up a readable stream, a transform stream, and a writable stream, then pipes them together:

```js
rs.pipeThrough(ts).pipeTo(ws);
```

It measures two metrics:

- the total time it takes for all data to make it from the readable stream's underlying source to the writable stream's underlying sink;
- the number of "pauses," i.e. times backpressure was exerted on the underlying source (i.e., the number of times `enqueue` returned `false`).

## The Varying Parameters

The potentially-varying parameters under which the above benchmark is run are found in `params.js`. They are:

- Number of chunks generated by the underlying source
- Size of the chunks generated by the underlying source
- The multiplier the transform stream applies to the chunks to produce new chunks
- The rate at which the underlying source produces chunks
- The rate at which the transform stream completes its transformation
- The rate at which the underlying sink acknowledges consumption of chunks
- The high-water mark of the readable stream
- The high-water mark of the transform stream's input side
- The high-water mark of the transform stream's output side
- The high-water mark of the writable stream

All rates are specified either in milliseconds, or as happening synchronously. Chunk size and the high-water marks are specified in bytes.

Due to the desire to have these tests run in a reasonable amount of time (approximately two minutes), the number of choices for each parameter is limited, and the rates were kept low. If you can think of ways to tweak the parameters to get better coverage without too much extra run-time, that would be much appreciated.

## Running the Benchmark

To run the benchmark, just do `npm run bench`. This will generate a result in the `results` folder. If you have previously run the benchmark, the command will then print out any cases that were different between the current run and those previous ones (discarding differences below the epsilon of 15 ms). For example, the output might be

```
20140815T003223313Z.json (current) vs. 20140815T002429998Z.json (current - 3)
-----------------------------------------------------------------------------
16,1024,15,4096,sync,0.3,0,0,0,15    +19.014197  0
1,1024,5,4096,5,0.3,0,0,0,15         -15.244131  0


20140815T003223313Z.json (current) vs. 20140815T002758672Z.json (current - 2)
-----------------------------------------------------------------------------
16,1024,15,4096,0,2,0,0,4096,0       +6.975208   +2
16,1024,15,4096,0,2,0,0,0,5          -23.327922  -1
16,1024,0,0,sync,0.3,0,0,4096,15     -21.750097  0


20140815T003223313Z.json (current) vs. 20140815T003011172Z.json (current - 1)
-----------------------------------------------------------------------------
(No difference within the epsilon)
```

Here the columns are: parameter values for the test case that differed; milliseconds difference between the two; and the delta in the number of pauses in each case. Positive numbers mean the current run is slower/more pausy than the previous ones.

## Under Construction

This is a work in progress.

As mentioned above, the parameter ranges could be broader. More scenarios could be tested. Streams whose behavior changes over the course of time (in a deterministic way) could be introduced. These ideas would increase running time, but perhaps, for larger changes, that is worth it?

The runner could be improved. Right now we're keeping the epsilon cutoffs low, so as not to miss anything. But this means that even without changing the code, differences will be printed between runs. We could raise the epsilons, or perhaps better, we could run each case a few times and average the results.

Mainly, we've yet to use this in anger. We'll see how well it works out as we continue developing.
